---
title: "Project 2 - Regression and CART"
author: "Group 3"
date: "3/13/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Group Leader: \underline{Brianna Johnson}

Member Names: \underline{Marco Sousa, Ben Pfeffer, Nikita Seleznev, Brianna Johnson}


# Introduction to the Heart Dataset

```{r libraries, include=FALSE}
#Visualization
library(ggplot2)
#Utilities
library(dplyr)
#Regression
library(ISLR)
#correlations
library(corrplot)
# library to PLOT ROC
library(pROC)
```

Observing the data:
```{r loadingData, echo=FALSE}

#Importing from directory
data <- read.csv("https://www.statlearning.com/s/Heart.csv ")

#Dropping first bogus column
data = data[-c(1)]

#Constructs a new column replacing No with 0 and Yes with 1
data <- data %>%
      mutate(AHDBinary = ifelse(AHD == "No",0,1))

#ca has a few na, so we may or may not remove them, yet simply state so.
data <- na.omit(data)

head(data)
```

Describing the data, and (1) identify Y and X.

The Heart dataset considers AHD, their binary relationship of having heart disease or not, and their other associated properties. The dataset contains 303 observations and 14 attributes.

More to be added/edited


# Exploratory Data Analysis

## Count of Binary Outcome

The following is a simple barplot of the count for the binary AHD outcome. We can see there are some more "No", than "Yes".

```{r binaryCount, echo=FALSE,fig.width=5, fig.height=4,fig.align='center'}

AHDbar <- ggplot(data, aes(AHD)) + geom_bar(fill="lightblue",color="black")+ ggtitle("Count of AHD") + theme(plot.title = element_text(hjust = 0.5, size = 17)) + geom_text(stat='count', aes(label=..count..), vjust=10)

AHDbar
```

## Correlation matrix

No correlation among AHD Binary exceeds 0.5, naturally.

```{r Corrmatrix, echo=FALSE,out.width='70%',align='center'}

#Removing na and removing non-numeric categorical data
tempData = subset(data, select= -c(AHD,ChestPain,Thal))
tempData <- na.omit(tempData)

#covariance then correlation plot
corMatrix <- cor(tempData)
corrplot.mixed(corMatrix, number.cex= 9/ncol(data),tl.cex= 9/ncol(data),lower.col = "black")

```


# Logistic Model

## Logistic Model

Constructing a logistic model for AHD based on other features. training and testing sets were separated randomly in a 70/30 split. To construct a final logistic model, features with the highest p values were removed until the remaining attributes were all significant. Some summaries were skipped over for brevity.

Splitting the data and setting seed.
```{r dataSplit, echo=TRUE}

set.seed(1)

indices <-sample(1:nrow(data), 0.7 * nrow(data), replace = TRUE)

training <-data[indices,]
test  <-data[-indices,]

```

Carrying out the feature selection and yielding a final regression model.

```{r logisticModel, echo=TRUE}

# Entire glm fit for numeric data
glm.fit <- glm(AHDBinary  ~ Age+Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Oldpeak+Slope+Ca, data = training, family = binomial)
summary(glm.fit)

# Considering non-numeric categorical data
glm.fit.cat <- glm(AHDBinary  ~ChestPain+Thal, data = training, family = binomial)
summary(glm.fit.cat)

# Altogether
glm.fit <- glm(AHDBinary  ~ Age+Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Oldpeak+Slope+Ca+ChestPain+Thal, data = training, family = binomial)
summary(glm.fit)

# Removing highest param (Oldpeak)
glm.fit <- glm(AHDBinary  ~ Age+Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)
summary(glm.fit)

# Removing highest param (Age?!)
glm.fit <- glm(AHDBinary  ~ Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)

# Removing highest param (RestECG)
glm.fit <- glm(AHDBinary  ~ Sex+RestBP+Chol+Fbs+MaxHR+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)

# Removing highest params (MaxHR)
glm.fit <- glm(AHDBinary  ~ Sex+RestBP+Chol+Fbs+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)

# Removing rest of nonsignificant params in order (skipping ahead)
glm.fit5 <- glm(AHDBinary  ~ Sex+Chol+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)
summary(glm.fit5)
```



## Confusion Table

Constructing probability distribution for predictions on each test observation.
```{r probDistribution, echo=TRUE}

glm.probs = predict(glm.fit5, test, type = "response")
#The first 10 predicted probabilities
glm.probs[1:10]

#Visualizing our probability distribution
colors <- c(rep("red",10), rep("green",10))
probHist = ggplot(mapping = aes(glm.probs)) + geom_histogram(binwidth=0.05,boundary = 0,color="black", fill=colors)
probHist = probHist + ggtitle("Histogram of Probabilities") + theme(plot.title = element_text(hjust = 0.5, size = 17))
probHist
```

Generate confusion table based off of 0.5 prediction cutoff.

```{r confusionTable, echo=TRUE}
#Choosing 0.5 as the cutoff for prediction
glm.pred <- ifelse(glm.probs > 0.5,1,0)
#Constructing the table
glm.table = table(glm.pred,test$AHDBinary)
glm.table
```

 
## Mode-Test Statistics

Calculate classification accuracy and error, sensitivity, specificity, PPV and NPV.

```{r statistics}

#Accuracy
table.trace = sum(diag(glm.table))
table.sum = sum(glm.table)
acc = table.trace / table.sum
acc

#0.8754209

#error
err = 1 - acc
err

#sensitivity
sens = glm.table[1]/(glm.table[1] + glm.table[2])
sens

#Specificity
spec = glm.table[4]/(glm.table[4] + glm.table[3])
spec

#PPV - Positive Predictive Value
PPV = glm.table[1]/(glm.table[1] + glm.table[3])
PPV

#NPV - Negative Predictive Value
PPV = glm.table[4]/(glm.table[4] + glm.table[2])
PPV

```



## ROC and AUC

Generate ROC and compute AUC for each model


```{r logisticRegression13}

test_prob = predict(glm.fit5, newdata = test, type = "response")

test_roc = roc(test$AHDBinary, test_prob)

plot.roc(test_roc, col=par("fg"),print.auc=FALSE,legacy.axes=TRUE,asp =NA)

plot.roc(smooth(test_roc),col="blue",add=TRUE,print.auc=TRUE,legacy.axes = TRUE, asp =NA)
legend("bottomright",legend=c("Empirical","Smoothed"),col=c(par("fg"),"blue"), lwd=2)

abline(v = -coef(glm.fit5)[1] / coef(glm.fit5)[2], lwd = 3)
```

## S sigmoid curve

Generate s-curve for Y against one attribute (you can pick any one attribute), and interpret your findings

I used RestBP TEMPORARILY. THis will be replaced with something more appropriate when we try our CART model.
```{r sCurve,echo=TRUE}

single.glm <- glm(AHDBinary ~ RestBP, data = data, family = "binomial")

plot(AHDBinary ~ RestBP, data = data,col = "darkorange", pch = "|", xlim = c(0, 2500), ylim = c(0, 1),main = "Using Logistic Regression for Classification")
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)

curve(predict(single.glm, data.frame(RestBP = x), type = "response"),add = TRUE, lwd = 3, col = "dodgerblue")
abline(v = -coef(single.glm)[1] / coef(single.glm)[2], lwd = 3)


```


# CART Model

## CART MODEL

## Other subsections

# Other Models (Bagging, Random Forests, Boosting)

```{r dataSplitOM, echo=TRUE}
Heart <-read.csv('Heart.csv')
Heart <- na.omit(Heart) #Remove NA for demo
data <- Heart[,-1]
set.seed(490)
split <- sample(1:nrow(data), size=nrow(data) * 0.7)
train <- data[split,]
test <- data[-split,]
```

```{r libraryNew}
library(randomForest)
```

## Bagged Trees

```{r Bagged Trees}

train$AHD = factor(train$AHD)

bag.class <- randomForest(AHD~., data = train, mtry=ncol(train) - 1, importance=TRUE,xtest=test[,-14],ytest=factor(test$AHD))

err <- bag.class$err.rate[,1]
bag.err <- cbind(err, bag.class$test$err.rate[,1])
colnames(bag.err) <- c("OOB", "Test")
matplot(1:bag.class$ntree, bag.err, type = "l", xlab="trees", ylab="Error", col = c("red", "blue"))
legend("right", c('OOB', 'Test') ,col=seq_len(2),cex=0.8,fill=c("red", "blue"))

```

## Random Forests
```{r randomForest1}

# setting mtry to sqrt(p) is a rule of thumb, this number can be set by
# k fold CV as well
rf.class <- randomForest(AHD~., data=train, mtry=round(sqrt(ncol(train) - 1)), importance=TRUE)
#importance(rf.class)
varImpPlot(rf.class)
```


```{r randomForests2}
plot(rf.class, col=c("red", "black", "blue"))
legend("top", colnames(rf.class$err.rate) ,col=seq_len(3),cex=0.8,fill=c("red", "black", "blue"))
```


```{r randomForest3}
#Try choosing mtry by plotting the OOB error
p <- ncol(train) - 1
oob.error.class <- double(p) #initialize empty vector
set.seed(1)
for(m in 1:p) {
fit <- randomForest(AHD ~ ., data=train, mtry=m, ntree=175)
conf.mat <- fit$err.rate[175]
oob.error.class[m] <- fit$err.rate[175, 'OOB']
}
matplot(1:p, oob.error.class, pch=19, col="red", type="b", ylab="Misclassification Error", xlab="mtry")
```


```{r randomForests4}

# setting mtry to sqrt(p) is a rule of thumb, this number can be set by
# k fold CV as well
rf.class <- randomForest(AHD~., data=train, mtry=3, importance=TRUE)
plot(rf.class, col=c("red", "black", "blue"))
legend("top", colnames(rf.class$err.rate) ,col=seq_len(3),cex=0.8,fill=c("red", "black", "blue"))

```

## Boosting

```{r libraryBoosting}
library(gbm)
```

```{r Boosting}
set.seed(1)
#format y for gbm, must be 0/1
train$ChestPain = as.numeric(factor(train$ChestPain))
train$Thal = as.numeric(factor(train$Thal))
AHD.0.1 <- ifelse(train$AHD == 'Yes', 1, 0)
class.boost = gbm(AHD.0.1 ~ . - AHD, data = train, n.trees = 5000, distribution = "adaboost", shrinkage = 0.01)
summary(class.boost)
```

# Comparing Models


# Citations



