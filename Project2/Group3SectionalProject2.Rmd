---
title: "Project 2 - Regression and CART"
author: "Group 3"
date: "3/13/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Group Leader: \underline{Brianna Johnson}

Member Names: \underline{Marco Sousa, Ben Pfeffer, Nikita Seleznev, Brianna Johnson}

# Introduction to the Heart Dataset

```{r libraries, include=FALSE}
#Visualization
library(ggplot2)
#Utilities
library(dplyr)
#Regression
library(ISLR)
#correlations
library(corrplot)
# library to PLOT ROC
library(pROC)
#CART Stuff
library(rpart)
library(rpart.plot)

```

Observing the data:
```{r loadingData, echo=FALSE}

#Importing from directory
data <- read.csv("https://www.statlearning.com/s/Heart.csv ")

#Dropping first bogus column
data = data[-c(1)]

#Constructs a new column replacing No with 0 and Yes with 1
data <- data %>%
      mutate(AHDBinary = ifelse(AHD == "No",0,1))

#ca has a few na, so we may or may not remove them, yet simply state so.
data <- na.omit(data)

head(data)
```


The 'Heart' dataset considers AHD, the binary relationship of having heart disease or not, and other perhaps associated features. The dataset contains 303 observations and 14 attributes. Some attributes like MaxHR and RestBP are numerical, yet others like Thal or ChestPain are categorical. The direct link to the data from statlearning can be found [here](https://www.statlearning.com/s/Heart.csv).

In particular, we are interested in constructing a model that correctly predicts whether AHD will be "Yes", or "No", as a manner of classification, based on other presented features of the data. We investigate three techniques of carrying out such a classification task: logistic regression, CART, and random forests.

The data is displayed as follows.
```{r showData}
head(data)
```

# Exploratory Data Analysis

## Count of Binary Outcome

The following is a simple barplot of the count for the binary AHD outcome. We can see there are some more "No", than "Yes". More precisely, there are 160 "No", and 137 "Yes". 

```{r binaryCount, echo=FALSE,fig.width=5, fig.height=3.2,fig.width=3.2,fig.align='center'}
AHDbar <- ggplot(data, aes(AHD)) + geom_bar(fill="lightblue",color="black")+ ggtitle("Count of AHD") + theme(plot.title = element_text(hjust = 0.5, size = 17)) + geom_text(stat='count', aes(label=..count..), vjust=10)

AHDbar
```

\newpage

## Correlation matrix

No correlation among AHD Binary exceeds 0.5, naturally.

```{r Corrmatrix, echo=FALSE,out.width='80%',align='center'}

#Removing na and removing non-numeric categorical data
tempData = subset(data, select= -c(AHD,ChestPain,Thal))
tempData <- na.omit(tempData)

#covariance then correlation plot
corMatrix <- cor(tempData)
corrplot.mixed(corMatrix, number.cex= 9/ncol(data),tl.cex= 9/ncol(data),lower.col = "black")

```


# Logistic Model

A logistic model is a standard machine learning model used to predict the probability of a binary event, like win lose, yes no, and so on. This does so, briefly speaking, by using the logit transofrmation of the odds to activate our logistic regression formula, resembling p(event)=$\frac{e^{X\beta}}{1+e^{X\beta}}$. In this application, we are interested in calculating a probability for "Yes" versus "No" (or 1 vs 0) for AHD using the other data features.

## Logistic Model

Constructing a logistic model for AHD based on other features. Training and testing sets were separated randomly in a 70/30 split. To construct a final logistic model, features with the highest p values were removed until the remaining attributes were all significant. Some summaries were skipped over for brevity.

Splitting the data and setting seed.
```{r dataSplit, echo=TRUE}

set.seed(1)

indices <-sample(1:nrow(data), 0.7 * nrow(data), replace = TRUE)
training <-data[indices,]
test  <-data[-indices,]
```

Carrying out the feature selection and yielding a final regression model.

```{r logisticModel, echo=TRUE}

# Entire glm fit for numeric data
glm.fit <- glm(AHDBinary  ~ Age+Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Oldpeak+Slope+Ca, data = training, family = binomial)
summary(glm.fit)

# Considering non-numeric categorical data
glm.fit.cat <- glm(AHDBinary  ~ChestPain+Thal, data = training, family = binomial)
summary(glm.fit.cat)

# Altogether
glm.fit <- glm(AHDBinary  ~ Age+Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Oldpeak+Slope+Ca+ChestPain+Thal, data = training, family = binomial)
summary(glm.fit)

# Removing highest param (Oldpeak)
glm.fit <- glm(AHDBinary  ~ Age+Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)
summary(glm.fit)

# Removing highest param (Age?!)
glm.fit <- glm(AHDBinary  ~ Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)

# Removing highest param (RestECG)
glm.fit <- glm(AHDBinary  ~ Sex+RestBP+Chol+Fbs+MaxHR+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)

# Removing highest params (MaxHR)
glm.fit <- glm(AHDBinary  ~ Sex+RestBP+Chol+Fbs+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)

# Removing rest of nonsignificant params in order (skipping ahead)
glm.fit5 <- glm(AHDBinary  ~ Sex+Chol+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)
summary(glm.fit5)
```

Note that the removals were not biased by intuition as to the viability of the feature. That is, they were simply removed depending on their controbution to the model, bot on their intuitive viability to the domain.

## Confusion Table

Constructing probability distribution for predictions on each test observation. The following is simply an example of the first ten prediction probabilities. We can observe, for these, that many are very high (~99\%).
```{r probDistribution, echo=TRUE}

glm.probs = predict(glm.fit5, test, type = "response")
#The first 10 predicted probabilities
glm.probs[1:10]
```

The following is a distribution of these probabilities. Green represents the probabilities that are greater than 50\%, and thus will be classified for "Yes". A promising component of our model is that the distribution is skewed towards 1 and 0, rather than being more uniform. This implies there is a decent distinction between yes or no in producing our prediction, in addition to accuracy analysis.

```{r probDistributionHistogram}
#Visualizing our probability distribution
colors <- c(rep("red",10), rep("green",10))
probHist = ggplot(mapping = aes(glm.probs)) + geom_histogram(binwidth=0.05,boundary = 0,color="black", fill=colors)
probHist = probHist + ggtitle("Histogram of Probabilities") + theme(plot.title = element_text(hjust = 0.5, size = 17))+xlab("Probability value") + ylab("Count")+theme(aspect.ratio=1)
probHist
```

Generate confusion table based off of 0.5 prediction cutoff.

```{r confusionTable, echo=TRUE}
#Choosing 0.5 as the cutoff for prediction
glm.pred <- ifelse(glm.probs > 0.5,1,0)
#Constructing the table
glm.table = table(glm.pred,test$AHDBinary)
glm.table
```

 
## Mode-Test Statistics

Calculate classification accuracy and error, sensitivity, specificity, PPV and NPV. We indeed saw more "no" than "Yes" (represented as 0 and 1 respectively). We also seemed to have about equal PPV and NPV.

```{r statistics}

#Accuracy
table.trace = sum(diag(glm.table))
table.sum = sum(glm.table)
acc = table.trace / table.sum
acc

#0.8754209

#error
err = 1 - acc
err

#sensitivity
sens = glm.table[1]/(glm.table[1] + glm.table[2])
sens

#Specificity
spec = glm.table[4]/(glm.table[4] + glm.table[3])
spec

#PPV - Positive Predictive Value
PPV = glm.table[1]/(glm.table[1] + glm.table[3])
PPV

#NPV - Negative Predictive Value
PPV = glm.table[4]/(glm.table[4] + glm.table[2])
PPV

```


## ROC and AUC

Generate ROC and compute AUC. The AUC is displayed on the figure below, that being 0.903, generated by integrating and similarly calculating the area under the curve. The ROC and AUC gives us more insight as to the performance of our model. An AUC of 0.903 is fairly acceptable performance, and indicates that there is a class separation between yes and no.

```{r logisticRegression13}

test_prob = predict(glm.fit5, newdata = test, type = "response")

test_roc = roc(test$AHDBinary, test_prob)

plot.roc(test_roc, col=par("fg"),print.auc=FALSE,legacy.axes=TRUE,asp =NA)

plot.roc(smooth(test_roc),col="blue",add=TRUE,print.auc=TRUE,legacy.axes = TRUE, asp =NA)
legend("bottomright",legend=c("Empirical","Smoothed"),col=c(par("fg"),"blue"), lwd=2)

abline(v = -coef(glm.fit5)[1] / coef(glm.fit5)[2], lwd = 3)
```


## S sigmoid curve

The following is a s-curve for Chol regarding our logistic model. The orange lines represent the points for Chol with AHDBinary (Chol,AHDBinary). The blue "s-curve" represent the predicted probabilities given the fitted logistic regression. The black line represents the 0.5 cutoff, and decision boundary.

In our case, Chol did not seem to fit our s-curve very well. Firstly, our so called s curve doesn't immediately seem to resemble a s-curve. As such, our points are not well fitted to the curve. Furthermore, 0 and 1 points lie on both sides of the dicision boundary. Chol was one of the paramaters that we included which happened to be significant within our model and random train/test split. Looking at the orange locations more closely, we can see some high cholesterol values, yet belong in the "no" category, which are interesting points that pull down on that end of the s curve. 

This runs counter to what we may expect. It could be expected that the "yes" category would be full of higher cholesterol, yet the "No" category to have lower cholesteral values in contrast.

```{r sCurve,echo=TRUE}

single.glm <- glm(AHDBinary ~ Chol, data = data, family = "binomial")


plot(AHDBinary ~ Chol, data = data,col = "darkorange", pch = "|", xlim = c(0, 600), ylim = c(0, 1),main = "Using Logistic Regression for Classification")
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)

curve(predict(single.glm, data.frame(Chol = x), type = "response"),add = TRUE, lwd = 3, col = "dodgerblue")
abline(v = -coef(single.glm)[1] / coef(single.glm)[2], lwd = 3)

```

# CART Model

A Classification and Regression Tree (CART) can be used as a predictive model that explains how a combination of variables can work together, and output a singular result. Specifically, in our Heart Disease Dataset, we are able to determine how variables interact with one another, to determine the binary outcome of having Heart Disease or not. In order to do this, we first obtain our Decision Tree, a series of if/else statements that will lead to a “yes” or “no” prediction. This CART Model serves as a more probabilistic scenario that occurs when dealing with a patient’s medical history. By analyzing factors such as Heart Rate, Blood Pressure, and age, doctor’s can better assess the health of a patient, and make a determination into the likelihood of them having Heart Disease. 

Due to the nature of our dataset, we will specifically model Classification Trees, as we are dealing with a binary outcome of “Yes, the patient has AHD”, or “No, the patient does not have AHD”. Although initially similar to Regression Trees, we will begin with a recursive binary splitting to grow our tree, but do not need to rely on repeating to minimize the SSR/SSE/RSS. Rather, we are more focused on our Classification Error Rate and minimizing our Gini index. Although the CART model may not have the same level of predictive accuracy as some of the other tested approaches in this report, it is the most similar to mirror human decision-making. Thus, it is a strong predictive model for our specific dataset on Heart Disease. 


## CART MODEL

```{r BCARTSplittingData}

Heart <-read.csv('https://www.statlearning.com/s/Heart.csv')
set.seed(1)
heart.split <- sample(1:nrow(Heart), size=nrow(Heart) * 0.7)
heart.train <- Heart[heart.split,]
heart.test <- Heart[-heart.split,]

```

Constructing our splits.

```{r BCART2}
class.cart <- rpart(formula = AHD ~ Age+Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Oldpeak+Slope+Ca+ChestPain+Thal, data = heart.train, method = "class", control = rpart.control(minbucket = 2, xval = 10))
prp(class.cart, roundint = FALSE)

```

Based on the results of the above Decision Tree, Thalassemia and Chest Pain was determined as the leading predictor of Heart Disease, which was also seen, through the above logistic model. Other interesting patterns to note, would be the classification of age between 51-62, for individuals with Normal Thalassemia and no Chest Pain. 62 became a cutoff point for patients as an indication that their Chest Pain and Thalassemia would not be a factor of Heart Disease, while Chest Pain, without Thalassemia was mostly present in those over the age of 66. 

```{r BCART3}
cp.class.param <- class.cart$cptable
train.acc <- double(6)
cv.acc <- double(6)
test.acc <- double(6)
for (i in 1:nrow(cp.class.param)) {
  alpha <- cp.class.param[i, 'CP']
  train.cm <- table(heart.train$AHD, predict(prune(class.cart, cp=alpha), newdata = heart.train, type='class'))
  train.acc[i] <- 1-sum(diag(train.cm))/sum(train.cm)
  cv.acc[i] <- cp.class.param[i, 'xerror'] * cp.class.param[i, 'rel error']
  test.cm <- table(heart.test$AHD, predict(prune(class.cart, cp=alpha), newdata = heart.test, type='class'))
  test.acc[i] <- 1-sum(diag(test.cm))/sum(test.cm)
}

```

Train cv and test accuracy.

```{r BCART4}
matplot(cp.class.param[,'nsplit'], cbind(train.acc, cv.acc, test.acc), pch=19, col=c("red", "black", "blue"), type="b", ylab="Loss", xlab="Depth")
legend("right", c('Train', 'CV', 'Test') ,col=seq_len(3),cex=0.8,fill=c("red", "black", "blue"))

```

Looking at the plot of Train, Test, and CV Values, a tree of size 5 seems to be most accurate.

Pruning.

```{r BCART5Prune}
prune.class.trees <- prune(class.cart, cp=cp.class.param[5,'CP'])
prp(prune.class.trees)

```

## Confusion Matrix
Confusion Matrix
```{r}
conf.mat.tree <- table(heart.test$AHD, predict(prune.class.trees, type = 'class', newdata = heart.test))
conf.mat.tree
```


## Mode-Test Statistics

Statistics regarding our confusion table.

```{r}
acc <- sum(diag(conf.mat.tree))/sum(conf.mat.tree)
err <- 1 - acc
sens <- conf.mat.tree[1,1]/(conf.mat.tree[1,1] + conf.mat.tree[2,1])
spec <- conf.mat.tree[2,2]/(conf.mat.tree[2,2] + conf.mat.tree[1,2])
ppv <- conf.mat.tree[1,1]/(conf.mat.tree[1,1] + conf.mat.tree[1,2])
npv <- conf.mat.tree[2,2]/(conf.mat.tree[2,2] + conf.mat.tree[2,1])
c(Accuracy = acc, Error = err, Sensitivity=sens, Specificity = spec, PPV = ppv, NPV = npv)
```

## ROC
```{r ROC}

library(rpart)
library(ROCR)

test_prob = predict(prune.class.trees, newdata = heart.test, type='class')
test_roc = roc(heart.test$AHD, factor(test_prob, ordered = TRUE))
plot.roc(test_roc, col=par("fg"),plot=TRUE,print.auc = FALSE, legacy.axes = TRUE, asp =NA)

#pred <- prediction(predict(class.cart, type="prob")[, 2], Heart$AHD)
#plot(performance(pred, "tpr", "fpr"), col="blue", main="ROC AHD")
#abline(0, 1, lty=2)

```



# Random Forests

The decision trees approach suffers from high variance, meaning  the results of the tree fitting to the raining set can be quite different depending on the training/test set split. To combat this issue with the decision trees bootstrap aggregation is employed, which is also referred to as "bagging". In this approach many training sets are derived from the population using bootstrap, a separate prediction model using each training set is developed, and the resulting predictions are averaged. For the classification task that we consider in this project instead of averaging the majority vote is taken across the predicted classes. This allows to reduce the variance of the statistical method. 

On average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. Testing the predictions on the OOB observations is the foundation for the OOB error estimate. Based on the results shown below approximately 230 trees are sufficient for both errors to stabilize. 

Random forests provide an improvement over bagged trees by way of a random tweak that decorrelates the trees. The split is allowed to use only one a subset of predictors. A fresh sample of predictors is taken at each split, and the number of predictors in the subset typically equals the square root of the total number of predictors. The random forest results and a variable importance plot for the Heart data are given below. 


```{r a}
Heart <-read.csv('Heart.csv')
Heart <- na.omit(Heart) #Remove NA for demo
data1 <- Heart[,-1]
set.seed(490)
split <- sample(1:nrow(data1), size=nrow(data1) * 0.7)
train <- data1[split,]
test <-  data1[-split,]
train$AHD = factor(train$AHD)

```


```{r b}
library(randomForest)
set.seed(1)
rf.class <- randomForest(AHD~., data=train, mtry=round(sqrt(ncol(train)-1)), importance=TRUE, xtest=test[,-14], ytest=factor(test$AHD))
plot(rf.class, col=c("red", "black", "blue"))
legend("top", colnames(rf.class$err.rate) ,col=seq_len(3),cex=0.8,
       fill=c("red", "black", "blue"))
```


## Confusion Table

Carry out prediction with the Random Forest model:
```{r c}

rf_classifier <- randomForest(AHD ~ ., data=train, ntree = 500, mtry=round(sqrt(ncol(train)-1)), importance=TRUE)
prediction_for_table <- predict(rf_classifier, test[,-14])
c.table <- table(observed=test[,14],predicted=prediction_for_table)
c.table
```

## Mode-Test Statistics

Calculate classification accuracy and error, sensitivity, specificity, PPV and NPV.

```{r d}

#Accuracy
table.trace = sum(diag(c.table))
table.sum = sum(c.table)
acc = table.trace / table.sum
acc

#0.8754209

#error
err = 1 - acc
err

#sensitivity
sens = c.table[1]/(c.table[1] + c.table[2])
sens

#Specificity
spec = c.table[4]/(c.table[4] + c.table[3])
spec

#PPV - Positive Predictive Value
PPV = c.table[1]/(c.table[1] + c.table[3])
PPV

#NPV - Negative Predictive Value
PPV = c.table[4]/(c.table[4] + c.table[2])
PPV

```
 Thus, the Random Forest classifier achieved the accuracy of ~ 84 % and corresponding error of ~ 16 %.

## ROC and AUC

Generate ROC and compute AUC for Random Forest


```{r e}

test_prob = predict(rf_classifier, newdata = test[,-14], type = "class")
#test_prob = predict(prune.class.trees, newdata = heart.test, type='class')
test_roc = roc(test$AHD, factor(test_prob, ordered = TRUE))
plot.roc(test_roc, col=par("fg"),plot=TRUE,print.auc = FALSE, legacy.axes = TRUE, asp =NA)

```

# Comparing Models

|           |Accuracy| sens | spec | PPV | NPV |
|-----------|-------|-------|-------|-------|------|
| Logistic  | 0.832 | 0.822 | 0.842 | 0.855 | 0.808 |
| CART      | 0.725 | 0.789 | 0.679 | 0.638 | 0.818 |
|Rand Forest| 0.844 | 0.826 | 0.868 | 0.895 | 0.785 |


The three diferent models produces different cccuracy. The Logistic model had a classification accuracy on its test set of approximately 83 percent. This was higher than the alternative CART model. However, as can be seen in the above figures, the CART model can be visualized with a tree as a tradeoff for this drop in accuracy. As a remedy for the drop in accuracy, a random forest model was produced, allowing for the same visualisation, yet also having the greatest test accuracy among the three models. the sensitivity and specificity yielded ROC that seemed to discern between our two classes, those being yes and no for AHD.


# Citations
[1] Index of An Introduction to Statistical Learning/Heart.csv, https://www.statlearning.com/s/Heart.csv.

[2] Fang, Julia. “CIS490_LS9_21S_Classification_Logistic&ROC&AUC.” MyCourses, 2021.

[3] Fang, Julia. “CIS490_LS10_21S_CART.” MyCourses, 2021.

[4] Fang, Julia. “R_logistic&ROC_21S.” MyCourses, 2021.

[5] Fang, Julia. “R_Trees_S21.” MyCourses, 2021.

[6] Fang, Julia. “Supplement_Reading_BaggingRandomForestBoosting.” MyCourses, 2021

[7] James, G., Witten, D., Hastie, T. and Tibshirani, R., 2013. An introduction to statistical learning (Vol. 112, p. 18). New York: springer.





