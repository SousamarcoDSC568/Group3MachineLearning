---
title: "Project 2 - Regression and CART"
author: "Group 3"
date: "3/13/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

Group Leader: \underline{Brianna Johnson}

Member Names: \underline{Marco Sousa, Ben Pfeffer, Nikita Seleznev, Brianna Johnson}


# Introduction to the Heart Dataset

```{r libraries, include=FALSE}
#Visualization
library(ggplot2)
#Utilities
library(dplyr)
#Regression
library(ISLR)
#correlations
library(corrplot)
# library to PLOT ROC
library(pROC)
#CART Stuff
library(rpart)
library(rpart.plot)

```

Observing the data:
```{r loadingData, echo=FALSE}

#Importing from directory
data <- read.csv("https://www.statlearning.com/s/Heart.csv ")

#Dropping first bogus column
data = data[-c(1)]

#Constructs a new column replacing No with 0 and Yes with 1
data <- data %>%
      mutate(AHDBinary = ifelse(AHD == "No",0,1))

#ca has a few na, so we may or may not remove them, yet simply state so.
data <- na.omit(data)

head(data)
```

Describing the data, and (1) identify Y and X.

The Heart dataset considers AHD, their binary relationship of having heart disease or not, and their other associated properties. The dataset contains 303 observations and 14 attributes.

More to be added/edited


# Exploratory Data Analysis

## Count of Binary Outcome

The following is a simple barplot of the count for the binary AHD outcome. We can see there are some more "No", than "Yes".

```{r binaryCount, echo=FALSE,fig.width=5, fig.height=4,fig.align='center'}

AHDbar <- ggplot(data, aes(AHD)) + geom_bar(fill="lightblue",color="black")+ ggtitle("Count of AHD") + theme(plot.title = element_text(hjust = 0.5, size = 17)) + geom_text(stat='count', aes(label=..count..), vjust=10)

AHDbar
```

## Correlation matrix

No correlation among AHD Binary exceeds 0.5, naturally.

```{r Corrmatrix, echo=FALSE,out.width='70%',align='center'}

#Removing na and removing non-numeric categorical data
tempData = subset(data, select= -c(AHD,ChestPain,Thal))
tempData <- na.omit(tempData)

#covariance then correlation plot
corMatrix <- cor(tempData)
corrplot.mixed(corMatrix, number.cex= 9/ncol(data),tl.cex= 9/ncol(data),lower.col = "black")

```


# Logistic Model

## Logistic Model

Constructing a logistic model for AHD based on other features. training and testing sets were separated randomly in a 70/30 split. To construct a final logistic model, features with the highest p values were removed until the remaining attributes were all significant. Some summaries were skipped over for brevity.

Splitting the data and setting seed.
```{r dataSplit, echo=TRUE}

set.seed(1)

indices <-sample(1:nrow(data), 0.7 * nrow(data), replace = TRUE)

training <-data[indices,]
test  <-data[-indices,]

```

Carrying out the feature selection and yielding a final regression model.

```{r logisticModel, echo=TRUE}

# Entire glm fit for numeric data
glm.fit <- glm(AHDBinary  ~ Age+Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Oldpeak+Slope+Ca, data = training, family = binomial)
summary(glm.fit)

# Considering non-numeric categorical data
glm.fit.cat <- glm(AHDBinary  ~ChestPain+Thal, data = training, family = binomial)
summary(glm.fit.cat)

# Altogether
glm.fit <- glm(AHDBinary  ~ Age+Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Oldpeak+Slope+Ca+ChestPain+Thal, data = training, family = binomial)
summary(glm.fit)

# Removing highest param (Oldpeak)
glm.fit <- glm(AHDBinary  ~ Age+Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)
summary(glm.fit)

# Removing highest param (Age?!)
glm.fit <- glm(AHDBinary  ~ Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)

# Removing highest param (RestECG)
glm.fit <- glm(AHDBinary  ~ Sex+RestBP+Chol+Fbs+MaxHR+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)

# Removing highest params (MaxHR)
glm.fit <- glm(AHDBinary  ~ Sex+RestBP+Chol+Fbs+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)

# Removing rest of nonsignificant params in order (skipping ahead)
glm.fit5 <- glm(AHDBinary  ~ Sex+Chol+ExAng+Slope+Ca+ChestPain+Thal, data = training, family = binomial)
summary(glm.fit5)
```



## Confusion Table

Constructing probability distribution for predictions on each test observation.
```{r probDistribution, echo=TRUE}

glm.probs = predict(glm.fit5, test, type = "response")
#The first 10 predicted probabilities
glm.probs[1:10]

#Visualizing our probability distribution
colors <- c(rep("red",10), rep("green",10))
probHist = ggplot(mapping = aes(glm.probs)) + geom_histogram(binwidth=0.05,boundary = 0,color="black", fill=colors)
probHist = probHist + ggtitle("Histogram of Probabilities") + theme(plot.title = element_text(hjust = 0.5, size = 17))
probHist
```

Generate confusion table based off of 0.5 prediction cutoff.

```{r confusionTable, echo=TRUE}
#Choosing 0.5 as the cutoff for prediction
glm.pred <- ifelse(glm.probs > 0.5,1,0)
#Constructing the table
glm.table = table(glm.pred,test$AHDBinary)
glm.table
```

 
## Mode-Test Statistics

Calculate classification accuracy and error, sensitivity, specificity, PPV and NPV.

```{r statistics}

#Accuracy
table.trace = sum(diag(glm.table))
table.sum = sum(glm.table)
acc = table.trace / table.sum
acc

#0.8754209

#error
err = 1 - acc
err

#sensitivity
sens = glm.table[1]/(glm.table[1] + glm.table[2])
sens

#Specificity
spec = glm.table[4]/(glm.table[4] + glm.table[3])
spec

#PPV - Positive Predictive Value
PPV = glm.table[1]/(glm.table[1] + glm.table[3])
PPV

#NPV - Negative Predictive Value
PPV = glm.table[4]/(glm.table[4] + glm.table[2])
PPV

```



## ROC and AUC

Generate ROC and compute AUC for each model


```{r logisticRegression13}

test_prob = predict(glm.fit5, newdata = test, type = "response")

test_roc = roc(test$AHDBinary, test_prob)

plot.roc(test_roc, col=par("fg"),print.auc=FALSE,legacy.axes=TRUE,asp =NA)

plot.roc(smooth(test_roc),col="blue",add=TRUE,print.auc=TRUE,legacy.axes = TRUE, asp =NA)
legend("bottomright",legend=c("Empirical","Smoothed"),col=c(par("fg"),"blue"), lwd=2)

abline(v = -coef(glm.fit5)[1] / coef(glm.fit5)[2], lwd = 3)
```

## S sigmoid curve

Generate s-curve for Y against one attribute (you can pick any one attribute), and interpret your findings

I used RestBP TEMPORARILY. THis will be replaced with something more appropriate when we try our CART model.
```{r sCurve,echo=TRUE}

single.glm <- glm(AHDBinary ~ RestBP, data = data, family = "binomial")

plot(AHDBinary ~ RestBP, data = data,col = "darkorange", pch = "|", xlim = c(0, 2500), ylim = c(0, 1),main = "Using Logistic Regression for Classification")
abline(h = 0, lty = 3)
abline(h = 1, lty = 3)
abline(h = 0.5, lty = 2)

curve(predict(single.glm, data.frame(RestBP = x), type = "response"),add = TRUE, lwd = 3, col = "dodgerblue")
abline(v = -coef(single.glm)[1] / coef(single.glm)[2], lwd = 3)


```


# CART Model

## CART MODEL

```{r dataSplittingforCART,include=FALSE}
set.seed(1)

data.split <- sample(1:nrow(data), size=nrow(data) * 0.7)
data.train <- data[data.split,]
data.test <- data[-data.split,]
```

```{r CARTModel,include=FALSE}
data.cart <- rpart(formula = AHDBinary ~ Age+Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Oldpeak+Slope+Ca+ChestPain+Thal, data = data.train, method = "anova", control = rpart.control(minbucket = 6))
prp(data.cart, roundint = FALSE)

```


```{r CARTContinued,include=FALSE}
cp.param <- data.cart$cptable
train.mse <- double(8)
cv.mse <- double(8)
test.mse <- double(8)

cp.param
```


```{r CART3,include=FALSE}

for (i in 1:8){
  alpha <- cp.param[i, 'CP']
  train.mse[i] <- mean((data.train$AHDBinary - predict(prune(data.cart, cp=alpha), newdata = data.train))^2)
  cv.mse[i] <- cp.param[i, 'xerror'] * cp.param[i, 'rel error']
  test.mse[i] <- mean((data.test$AHDBinary - predict(prune(data.cart, cp=alpha), newdata = data.test))^2)
}
```


```{r CART4, include=FALSE}
train.mse
cv.mse
test.mse
```


## CART MODEL

```{r BCARTSplittingData}

Heart <-read.csv('https://www.statlearning.com/s/Heart.csv')
set.seed(1)
heart.split <- sample(1:nrow(Heart), size=nrow(Heart) * 0.7)
heart.train <- Heart[heart.split,]
heart.test <- Heart[-heart.split,]

```

Constructing our splits

```{r BCART2}
class.cart <- rpart(formula = AHD ~ Age+Sex+RestBP+Chol+Fbs+RestECG+MaxHR+ExAng+Oldpeak+Slope+Ca+ChestPain+Thal, data = heart.train, method = "class", control = rpart.control(minbucket = 2, xval = 10))
prp(class.cart, roundint = FALSE)

```


```{r BCART3}
cp.class.param <- class.cart$cptable
train.acc <- double(6)
cv.acc <- double(6)
test.acc <- double(6)
for (i in 1:nrow(cp.class.param)) {
  alpha <- cp.class.param[i, 'CP']
  train.cm <- table(heart.train$AHD, predict(prune(class.cart, cp=alpha), newdata = heart.train, type='class'))
  train.acc[i] <- 1-sum(diag(train.cm))/sum(train.cm)
  cv.acc[i] <- cp.class.param[i, 'xerror'] * cp.class.param[i, 'rel error']
  test.cm <- table(heart.test$AHD, predict(prune(class.cart, cp=alpha), newdata = heart.test, type='class'))
  test.acc[i] <- 1-sum(diag(test.cm))/sum(test.cm)
}

```

Train cv and test accuracy.

```{r BCART4}
matplot(cp.class.param[,'nsplit'], cbind(train.acc, cv.acc, test.acc), pch=19, col=c("red", "black", "blue"), type="b", ylab="Loss", xlab="Depth")
legend("right", c('Train', 'CV', 'Test') ,col=seq_len(3),cex=0.8,fill=c("red", "black", "blue"))

```

Pruning
```{r BCART5Prune}
prune.class.trees <- prune(class.cart, cp=cp.class.param[5,'CP'])
prp(prune.class.trees)

```

## Confusion Matrix
Confusion Matrix
```{r}
conf.mat.tree <- table(heart.test$AHD, predict(prune.class.trees, type = 'class', newdata = heart.test))
conf.mat.tree
```


## Mode-Test Statistics

Statistics regarding our confusion table.

```{r}
acc <- sum(diag(conf.mat.tree))/sum(conf.mat.tree)
err <- 1 - acc
sens <- conf.mat.tree[1,1]/(conf.mat.tree[1,1] + conf.mat.tree[2,1])
spec <- conf.mat.tree[2,2]/(conf.mat.tree[2,2] + conf.mat.tree[1,2])
ppv <- conf.mat.tree[1,1]/(conf.mat.tree[1,1] + conf.mat.tree[1,2])
npv <- conf.mat.tree[2,2]/(conf.mat.tree[2,2] + conf.mat.tree[2,1])
c(Accuracy = acc, Error = err, Sensitivity=sens, Specificity = spec, PPV = ppv, NPV = npv)
```

## ROC
```{r ROC}

library(rpart)
library(ROCR)

test_prob = predict(prune.class.trees, newdata = heart.test, type='class')
test_roc = roc(heart.test$AHD, factor(test_prob, ordered = TRUE))
plot.roc(test_roc, col=par("fg"),plot=TRUE,print.auc = FALSE, legacy.axes = TRUE, asp =NA)

#pred <- prediction(predict(class.cart, type="prob")[, 2], Heart$AHD)
#plot(performance(pred, "tpr", "fpr"), col="blue", main="ROC AHD")
#abline(0, 1, lty=2)

```




# Random Forests

The decision trees approach suffers from high variance, meaning  the results of the tree fitting to the raining set can be quite different depending on the training/test set split. To combat this issue with the decision trees bootstrap aggregation is employed, which is also referred to as "bagging". In this approach many training sets are derived from the population using bootstrap, a separate prediction model using each training set is developed, and the resulting predictions are averaged. For the classification task that we consider in this project instead of averaging the majority vote is taken across the predicted classes. This allows to reduce the variance of the statistical method. 

On average, each bagged tree makes use of around two-thirds of the observations. The remaining one-third of the observations not used to fit a given bagged tree are referred to as the out-of-bag (OOB) observations. Testing the predictions on the OOB observations is the foundation for the OOB error estimate. Based on the results shown below approximately 230 trees are sufficient for both errors to stabilize. 

Random forests provide an improvement over bagged trees by way of a random tweak that decorrelates the trees. The split is allowed to use only one a subset of predictors. A fresh sample of predictors is taken at each split, and the number of predictors in the subset typically equals the square root of the total number of predictors. The random forest results and a variable importance plot for the Heart data are given below. 


```{r a}
Heart <-read.csv('Heart.csv')
Heart <- na.omit(Heart) #Remove NA for demo
data1 <- Heart[,-1]
set.seed(490)
split <- sample(1:nrow(data1), size=nrow(data1) * 0.7)
train <- data1[split,]
test <-  data1[-split,]
train$AHD = factor(train$AHD)

```


```{r b}
library(randomForest)
set.seed(1)
rf.class <- randomForest(AHD~., data=train, mtry=round(sqrt(ncol(train)-1)), importance=TRUE, xtest=test[,-14], ytest=factor(test$AHD))
plot(rf.class, col=c("red", "black", "blue"))
legend("top", colnames(rf.class$err.rate) ,col=seq_len(3),cex=0.8,
       fill=c("red", "black", "blue"))
```


## Confusion Table

Carry out prediction with the Random Forest model:
```{r c}

rf_classifier <- randomForest(AHD ~ ., data=train, ntree = 500, mtry=round(sqrt(ncol(train)-1)), importance=TRUE)
prediction_for_table <- predict(rf_classifier, test[,-14])
c.table <- table(observed=test[,14],predicted=prediction_for_table)
c.table
```

## Mode-Test Statistics

Calculate classification accuracy and error, sensitivity, specificity, PPV and NPV.

```{r d}

#Accuracy
table.trace = sum(diag(c.table))
table.sum = sum(c.table)
acc = table.trace / table.sum
acc

#0.8754209

#error
err = 1 - acc
err

#sensitivity
sens = c.table[1]/(c.table[1] + c.table[2])
sens

#Specificity
spec = c.table[4]/(c.table[4] + c.table[3])
spec

#PPV - Positive Predictive Value
PPV = c.table[1]/(c.table[1] + c.table[3])
PPV

#NPV - Negative Predictive Value
PPV = c.table[4]/(c.table[4] + c.table[2])
PPV

```
 Thus, the Random Forest classifier achieved the accuracy of ~ 84 % and corresponding error of ~ 16 %.

## ROC and AUC

Generate ROC and compute AUC for Random Forest


```{r e}

test_prob = predict(rf_classifier, newdata = test[,-14], type = "class")
#test_prob = predict(prune.class.trees, newdata = heart.test, type='class')
test_roc = roc(test$AHD, factor(test_prob, ordered = TRUE))
plot.roc(test_roc, col=par("fg"),plot=TRUE,print.auc = FALSE, legacy.axes = TRUE, asp =NA)

```

# Comparing Models

|            |Accuracy| sens | spec | PPV | NPV |
|-----------|-------|-------|-------|-------|------|
| Logistic  | 0.832 | 0.822 | 0.842 | 0.855 | 0.808 |
| CART      | 0.725 | 0.789 | 0.679 | 0.638 | 0.818 |
|Rand Forest| 0.844 | 0.826 | 0.868 | 0.895 | 0.785 |



# Citations
[1] Index of An Introduction to Statistical Learning/Heart.csv, https://www.statlearning.com/s/Heart.csv.

[2] Fang, Julia. “CIS490_LS9_21S_Classification_Logistic&ROC&AUC.” MyCourses, 2021.

[3] Fang, Julia. “CIS490_LS10_21S_CART.” MyCourses, 2021.

[4] Fang, Julia. “R_logistic&ROC_21S.” MyCourses, 2021.

[5] Fang, Julia. “R_Trees_S21.” MyCourses, 2021.

[6] Fang, Julia. “Supplement_Reading_BaggingRandomForestBoosting.” MyCourses, 2021

[7] James, G., Witten, D., Hastie, T. and Tibshirani, R., 2013. An introduction to statistical learning (Vol. 112, p. 18). New York: springer.





