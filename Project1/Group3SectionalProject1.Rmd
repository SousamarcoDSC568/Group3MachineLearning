---
title: "Sectional Project 1 - Regression"
author: "Group 3"
date: "2/20/2021"
output: pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages('ggplot2')
library(ggplot2)
library(corrplot)

library(glmnet)
library(plotmo)

```

Group Leader: \underline{Brianna Johnson}

Member Names: \underline{Nikita Seleznev, Marco Sousa, Brianna Johnson, Ben Pfeffer}

# Introduction to the Boston Housing Dataset

```{r importingData, echo=FALSE}
#Importing from directory; alternatively add the file to the github folder and load.
data <-read.table('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data')

#Names (Note: The .names file contains metadata on attributes)
names(data) <-c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS","RAD","TAX","PTRATIO","B","LSTAT","MEDV")

#There weren't any NA (dim(data) retained)
data <- na.omit(data)
```

The Boston Housing dataset considers housing values and their associated properties in suburbs of Boston, Massachusetts. The dataset contains 506 observations and 14 attributes. We acquired the dataset from the Machine Learning Database (MLDB), found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/). In particular, we are interested in constructing a model through regression techniques to gain insight on housing values. As such, we will use the 13 features to model 'MEDV', the median value of owner occupied homes (in $1,000s). 
The following is metadata for for each attribute as given by the housing.names file:

1. CRIM:per capita crime rate by town
2. ZN:proportion of residential land zoned for lots over 25,000 sq.ft.
3. INDUS: proportion of non-retail business acres per town
4. CHAS: Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)
5. NOX: nitric oxides concentration (parts per 10 million)
6. RM: average number of rooms per dwelling
7. AGE: proportion of owner-occupied units built prior to 1940
8. DIS: weighted distances to five Boston employment centres
9. RAD: index of accessibility to radial highways
10. TAX: full-value property-tax rate per $10,000
11. PTRATIO: pupil-teacher ratio by town
12. B: 1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town
13. LSTAT: $\%$lower status of the population
14. MEDV: Median value of owner-occupied homes in $1000's

The data is displayed as follows.
```{r displayData, echo=FALSE}
head(data)
```

# Exploratory Data Analysis

## MEDV Distribution

```{r MEDVHistogram, echo=FALSE,out.width='70%'}
histMEDV = ggplot(data, aes(MEDV)) + geom_histogram(bins=30,color="darkblue", fill="lightblue")+ ggtitle("Median Value Distribution") + theme(plot.title = element_text(hjust = 0.5, size = 17)) 

histMEDV
```

The histogram demonstrates the values are not uniformly distributed. Rather, they follow a mostly normal distributions, with some outliers at the tail.


## Correlation Matrix


```{r Corrmatrix, echo=FALSE,out.width='70%'}
corMatrix <- cor(data)
corrplot.mixed(corMatrix, number.cex= 9/ncol(data),tl.cex= 9/ncol(data))

```

While we produce many correlation values, we are firstly interested in how each attribute correlates to MEDV. This is represented by the bottom row or last column. We can immediately see the binary CHAS attribute does not correlate strongly with MEDV. However, it can be seen that RM (0.7) and LSTAT (-0.74) correlate with MEDV stronger than other attributes. Furthermore, the correlation between RM and LSTAT is -0.61. Since they do not correlate very strongly with one another, we can select both as predictor attributes without too much concern of collinearity for their case. The greatest correlation is between RAD and TAX of 0.91, and an $R^2$ of 0.82 thusly. Including both of these may raise some concerns regarding the minimal collinearity assumption of linear regression for these features.

# Modelling and Regression

MedV takes the value for Y, along 13 feature attributes of the dataset, in the form of $Y = \beta_0+\beta_1x_1+...+\beta_nx_n$ + b.

## Multiple Linear Regression

We chose to simply split the data in a 60/40 split for multiple Linear Regression.

```{r dataSplitting, echo=TRUE}
#Setting seed for reproducible results
set.seed(1)

indices <-sample(1:nrow(data), 0.6 * nrow(data), replace = TRUE)
training <-data[indices,]
testing  <-data[-indices,]
```

Naively consider all features except the binary at onset for a MLR.
```{r multiModel1, echo=TRUE}
#Naively considering many attributes
multiModel1 <-lm(MEDV ~CRIM+ZN+INDUS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT, data=training)
summary(multiModel1)
```

Some initial insight is that LSTAT and RM indeed were strong predictors. Removing INDUS, AGE, and B, every attribute becomes a significant predictor, depending on alpha. Let's consider what happens if we remove RAD, which varies strongly with TAX.

```{r multiModel2, echo=FALSE}
multiModel2 <-lm(MEDV ~CRIM+ZN+NOX+RM+DIS+TAX+PTRATIO+LSTAT, data=training)
summary(multiModel2)
```

We can see that without RAD, TAX is no longer a strong predictor. As such, TAX adds predictive value in relation to RAD. The next model removes TAX and adds RAD back in.

```{r multiModel3, echo=FALSE}
#Considering all values that are considered 'significant' with RAD included
multiModel3 <-lm(MEDV ~CRIM+ZN+NOX+RM+DIS+RAD+PTRATIO+LSTAT, data=training)
summary(multiModel3)
```

MSE and RMSE for model 1 and 3.
```{r multiModelTesting, echo=TRUE}
multiPredictions <-predict(multiModel1, testing)
RSS <- sum((testing$MEDV - multiPredictions)^2)
MSE1 <- mean((testing$MEDV - multiPredictions)^2)
RMSE1 <- sqrt(MSE1)

MSE1
RMSE1

multiPredictions <-predict(multiModel3, testing)
RSS3 <- sum((testing$MEDV - multiPredictions)^2)
MSE3 <- mean((testing$MEDV - multiPredictions)^2)
RMSE3 <- sqrt(MSE3)

MSE3
RMSE3
```

It seems we are left with 8 features. Lasso and Ridge will give us further insight into parameter selection. 

It could also be noted that NOX has a much larger coefficient with larger error than the other predictors.

## Ridge Regression

```{r ridgeDataFormatting, echo=FALSE}
#Setting seed
set.seed(1)

#Formatting into matrix
data.mat <- model.matrix(MEDV ~ .-1, data=data)

#delineating columns/features
x <- data.mat[, 1:13]
y <- data[, 'MEDV']

```

Constructing a Ridge model.
```{r ridgeModelStart, echo=TRUE}
#lambda grid
grid <- 10^seq(6, -3, length=10)
#ridgeModel
ridge.mod <- glmnet(scale(x), y, alpha = 0, lambda = grid, thresh = 1e-2, standardize = TRUE)
#Plotting the ridge.mod
plot_glmnet(ridge.mod, xvar = "lambda", label = 13)
```

Considering a best lambda for the model and hyperparameter tuning.
```{r ridgeLambdaTuning, echo=TRUE}
cv.out <- cv.glmnet(x, y, alpha=0, nfolds = 10)
cv.out

plot(cv.out)

best.lambda <- cv.out$lambda.min
best.lambda
```

Considering coefficients for Ridge.
```{r ridgeCoeff, echo=TRUE}

#Viewing coefficients of scaled full ridge model
predict(ridge.mod, type="coefficients", s=best.lambda)

#Selecting above abs(1): NOX, DIS, PTRATIO, LSTAT, RM
newX = data.mat[, c(5,6,8,11,13)]
head(newX)

#Final ridge with all coefficients
ridge.final1 <- glmnet(x, y, alpha = 0, lambda = best.lambda, thresh=1e-2, standardsize = TRUE)
predict(ridge.final1, type="coefficients", s=best.lambda)

#Final ridge with only 5 coefficients
ridge.final2 <- glmnet(newX, y, alpha = 0, lambda = best.lambda, thresh=1e-2, standardsize = TRUE)
predict(ridge.final2, type="coefficients", s=best.lambda)
```

Considering MSE and RMSE for Ridge using all coefficients.
```{r ridgeError, echo=TRUE}

ridge.pred1 <- predict(ridge.final1, s=best.lambda, newx=x)
ridge.MSE = mean((ridge.pred1 -y)^2)
ridge.RMSE = sqrt(mean((ridge.pred1 - y)^2))

ridge.MSE
ridge.RMSE

```

Considering MSE and RMSE for Ridge using only 5 coefficients.
```{r ridgeError2, echo=TRUE}

ridge.pred2 <- predict(ridge.final2, s=best.lambda, newx=newX)
ridge.MSE = mean((ridge.pred2 -y)^2)
ridge.RMSE = sqrt(mean((ridge.pred2 - y)^2))

ridge.MSE
ridge.RMSE

```

Considering R squared for each model manually
```{r ridgeError3, echo=TRUE}

yBar = mean(y)
RSS1 = sum((ridge.pred1 - y)^2)
TSS1 = sum((y - yBar)^2)
rsq1 = 1 - (RSS1/TSS1) 
rsq1

RSS2 = sum((ridge.pred2 - y)^2)
TSS2 = sum((y - yBar)^2)
rsq2 = 1 - (RSS2/TSS2) 
rsq2
```

## Lasso Regression

The Lasso regression is using regularization method based on the $\l1$ norm of the regression coefficients vector. This regularization yields sparse models by forcing some regression coefficients exactly to zero when regularization parameter $\lambda$ is sufficiently large. Thus, Lasso regression performs variables selection and produces simpler models, which are easier to interpret.  
  
We use the same range of the regularization parameter, $\lambda$, values as for the Ridge regression, but sample this range more densely as it provided better results for selecting different numbers of non-zero input variables.
The values of the Lasso regression coefficients for various $\lambda$ are shown below. It is evident that with increasing $\lambda$  some of the regression coeffienets are forced to zero and the number of degrees of freedom is decreasing. Based on the Lasso variable selection results the most important predictors that have non-zero regression coefficients even for large  $\lambda$ are the RM and LSTAT variables. This observation is consistent with the conclusions of the exploratory data analysis section.  
 
```{r lasso1}
grid <- 10^seq(6, -3, length=100)
lasso.mod <- glmnet(scale(x), y) #default alpha=1
plot_glmnet(lasso.mod, xvar="lambda", label = 12)

```

We use a 10-fold cross-validation approach to select the optimum value of $\lambda$. More detailed discussion on the cross-validation algorithm for the Lasso regression is given in the section " K-Fold Cross Validation ". The minimum MSE is produced for $\lambda$ = 0.0307, which keeps 11 non-zero regression coefficients. 

```{r lasso2}
##cross-validation
lasso.cv.out <- cv.glmnet(scale(x), y, alpha=1, nfolds = 10)
lasso.cv.out
plot(lasso.cv.out)
```
The coefficients of the Lasso regression for the optimum values of $\lambda$ corresponding to the 11 non-zero regression coefficients is given below. We also plot the predicted home price values versus the true values and calculate the MSE and RMSE regression errors.

```{r lasso3}
#keep few predictors in lasso
lasso.best.lambda <- lasso.cv.out$lambda[which.max(lasso.cv.out$nzero == 11)]
#lasso.final <- glmnet(scale(x), y, alpha=1, lambda=grid)
lasso.final <- glmnet(x, y, alpha=1, lambda=grid)
predict(lasso.final, type="coefficients", s=lasso.best.lambda )

lasso.pred <- predict(lasso.final, s=lasso.best.lambda, newx=x)
print(paste('MSE:', mean((lasso.pred - y)^2)))
print(paste('RMSE:', sqrt(mean((lasso.pred - y)^2))))
plot(y, lasso.pred,  xlab="True home value, $K", ylab="Lasso prediction, $K",)


```

Considering the R squared for the 11 selector Lasso model.
```{r rSquaredManualLasso1}
yBar = mean(y)
lassoRSS = sum((lasso.pred - y)^2)
lassoTSS = sum((y - yBar)^2)
rsq = 1 - (lassoRSS/lassoTSS) 
rsq
```


Next we further reduced the number of non-zero regression coefficients to keep only 5 most important predictors. The coefficients of the Lasso regression and the plot of the predicted home price versus the true home prices are shown below. We also calculate the MSE and RMSE errors for the case of 5 predictors. As expected, the scatter of the plot and the MSE/RMSE errors are somewhat higher compared to the case of 11 predictors, but the Lasso model is still providing acceptable results in terms of predicting the home values based on only 5 predictors.   

```{r lasso4}
# check for fewer predictors
lasso.best.lambda <- lasso.cv.out$lambda[which.max(lasso.cv.out$nzero == 5)]
lasso.final <- glmnet(x, y, alpha=1, lambda=grid)
predict(lasso.final, type="coefficients", s=lasso.best.lambda )
lasso.pred <- predict(lasso.final, s=lasso.best.lambda, newx=x)
print(paste('MSE:', mean((lasso.pred - y)^2)))
print(paste('RMSE:', sqrt(mean((lasso.pred - y)^2))))
plot(y, lasso.pred, xlab="True home value, $K", ylab="Lasso prediction, $K",)
```

Considering the R squared for the 5 selector Lasso model.
```{r rSquaredManualLasso2}
yBar = mean(y)
lassoRSS = sum((lasso.pred - y)^2)
lassoTSS = sum((y - yBar)^2)
rsq = 1 - (lassoRSS/lassoTSS) 
rsq

```
## K-Fold Cross Validation 

In a standard for the analysis of the algorithm performace a part of the data is reserved as a test set and the rest of the data is used as a training set. This may lead to a highly variable test error depending on how the dataset was split. Moreover, the machine learning approaches tend to perform  worse when less data are used for training, which happens when a part of the data has to be reserved for the test set.     

For optimization of the Lasso regression we are employing a cross-validation (CV) technique as it helps to alleviate the above problems. Specifically, we use a K-fold cross validation where one fold is used as a test set while the remaining (K-1) folds are used as the training set. After a given data fold is tested the algorithm computes the obtained MSE for this fold and moves to another one. Thus, every data point in the set is a part of the training and the test set for some folds. The final K-fold CV mean squared error (MSE) error is computed as the mean of the MSE of individual folds. 

In practice K = 5 or 10 tend to provide the best results as they yield a test error that does not suffer from excessively high bias, nor from very high variance. We chose to use the 10-fold cross-validation approach (K = 10). 

The 10-fold CV MSE is computed for each value of $\lambda$ on the grid that we have defined. Finally, the CV algorithm  chooses the value of $\lambda$ that minimizes the 10-fold CV MSE. In developing models based on the Lasso approach we also used  higher values of $\lambda$ to select the 5 most important predictors. 


The final MSE's and RMSE's achieved by different regression approaches are given in the table below:

|            | MSE    | RMSE  |$R^2$|
|------------|--------|-------|-----|
| Multiple 12| 22.554 | 4.749 |0.720|
| Multiple  8| 23.025 | 4.798 |0.695|
| Ridge    13| 22.894 | 4.784 |0.728|
| Ridge     5| 25.096 | 5.009 |0.702|
| Lasso    11| 23.445 | 4.842 |0.722|
| Lasso     5| 28.250 | 5.315 |0.665|


The above results suggest that having more independent variables as an input to regression provides overall smaller MSE and RMSE errors. However, the difference in errors between the Ridge regression and Lasso regression with 11 input variables is small. Furthermore, Lasso regression with only 5 most significant predictors still yields  RMSE error, which is only 9.8 % higher compared to the results with 11 predictors. Thus, Lasso approach enables significant reduction in the model complexity and dataset requirements without significant loss of accuracy. 

## Explicit Regression Formula

The following display the regression formula more explicitly for each finalized model in which the parameters were tuned or selected. The coefficients for initial naive models without tuning can still be seen above as output.

### Multiple Linear Regression (8)

MEDV = (-0.12935)CRIM
+(0.03885)ZN
+(-18.55609)NOX
+(3.51728)RM
+(-1.31761)DIS
+(0.18688)RAD
+(-0.97901)PTRATIO
+(-0.63314)LSTAT
+39.92696

### Ridge Regression (5)

MEDV =
+(-12.6387422)NOX 
+(4.5085207)RM
+(-0.7280733)DIS
+(-1.0318123)PTRATIO
+(-0.5087480)LSTAT
+(29.4519872)

### Lasso Regression (5)

MEDV = 
+(0.130133129)CHAS 
+(3.895113886)RM
+(-0.630418762)PTRATIO
+(0.002297575)B
+(-0.497652218)LSTAT
+(15.156492728)

# Summary

The goal was to model MEDV, the median value of owner occupied homes (in $1,000s), based on given features. MEDV follows a skewed normal distribution with some outliers at its tail. Three regression techniques were performed to model the data: multiple linear regression, Ridge, and Lasso. It was observed that TAX only became a predictor in MLR when RAD was included in, so TAX was removed. Removing insignificant predictors (to alpha) left 8 features. Ridge was performed using all features. Then Ridge was modified to include the 5 greatest predictors. This was determined by taking the greatest absolute value of the coefficients when input was scaled. Lasso was performed with 11 then 5 predictors. Among all methods, LSTAT, RM, and PTRatio remained as significant predictors for MEDV. Having more independent variables as an input to regression provides overall smaller MSE and RMSE errors. However, the difference in errors between Multiple Ridge and Lasso, each with mostly all input variables (11, 12, or 13), was small. This changed by different amounts when hyperparamater tuning  or selection took place.

# Citations

[1] Fang, Julia. “CIS490_LS8_21S_LassoReg&amp;CrossValidation.” MyCourses, 2021,
[linked umassd pdf](https://umassd.umassonline.net/bbcswebdav/pid-700293-dt-content-rid-4661573_1/xid-4661573_1)

[2] Fang, Julia. “R_RidgeLassoCV_Final.pdf” MyCourses, 2021, [linked umassd pdf](https://umassd.umassonline.net/bbcswebdav/pid-700293-dt-content-rid-4655072_1/xid-4655072_1).

[3]Index of /Ml/Machine-Learning-Databases/Housing, [archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/).
