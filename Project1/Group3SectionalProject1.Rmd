---
title: "Sectional Project 1 - Regression"
author: "Group 3"
date: "2/20/2021"
output: pdf_document
urlcolor: blue
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

#install.packages('ggplot2')
library(ggplot2)
library(corrplot)

library(glmnet)
library(plotmo)

```

Group Leader: \underline{Brianna Johnson}

Member Names: \underline{Marco Sousa, Nikita Seleznev, Ben Pfeffer, Brianna Johnson}

# Introduction to the Boston Housing Dataset


```{r importingData, echo=FALSE}
#Importing from directory; alternatively add the file to the github folder and load.
data <-read.table('https://archive.ics.uci.edu/ml/machine-learning-databases/housing/housing.data')

#Names (Note: The .names file contains metadata on attributes)
names(data) <-c("CRIM", "ZN", "INDUS", "CHAS", "NOX", "RM", "AGE", "DIS","RAD","TAX","PTRATIO","B","LSTAT","MEDV")

#There weren't any NA (dim(data) retained)
data <- na.omit(data)
```

The Boston Housing dataset considers housing values and their associated properties in suburbs of Boston, Massachusetts. The dataset contains 506 observations and 14 attributes. We acquired the dataset from the Machine Learning Database (MLDB), found [here](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/). In particular, we are interested in constructing a model through regression techniques to gain insight on housing values. As such, we will use the 13 features to model 'MEDV', the median value of owner occupied homes (in $1,000s). 

The data is displayed as follows.
```{r displayData, echo=FALSE}
head(data)
```

\newpage

# Exploratory Data Analysis

## MEDV Distribution

```{r MEDVHistogram, echo=FALSE,out.width='70%'}
histMEDV = ggplot(data, aes(MEDV)) + geom_histogram(bins=30,color="darkblue", fill="lightblue")+ ggtitle("Median Value Distribution") + theme(plot.title = element_text(hjust = 0.5, size = 17)) 

histMEDV
```

The histogram demonstrates the values are not uniformly distributed. Rather, they follow a mostly normal distributions, with some outliers at the tail.


## Correlation Matrix


```{r Corrmatrix, echo=FALSE,out.width='70%'}
corMatrix <- cor(data)
corrplot.mixed(corMatrix, number.cex= 9/ncol(data),tl.cex= 9/ncol(data))

```

While we produce many correlation values, we are firstly interested in how each attribute correlates to MEDV. This is represented by the bottom row or last column. We can immediately see the binary CHAS attribute does not correlate strongly with MEDV. However, it can be seen that RM (0.7) and LSTAT (-0.74) correlate with MEDV stronger than other attributes. Furthermore, the correlation between RM and LSTAT is -0.61. Since they do not correlate very strongly with one another, we can select both as predictor attributes without too much concern of collinearity for their case. The greatest correlation is between RAD and TAX of 0.91, and an $R^2$ of 0.82 thusly. Including both of these may raise some concerns regarding the minimal collinearity assumption of linear regression.

# Modelling and Regression

MedV takes the value for Y, along 13 feature attributes of the dataset, in the form of $Y = \beta_0+\beta_1x_1+...+\beta_nx_n$ + b.

## Multiple Linear Regression

We chose to simply split the data in a 60/40 split for multiple Linear Regression.

```{r dataSplitting, echo=TRUE}
#Setting seed for reproducible results
set.seed(1)

indices <-sample(1:nrow(data), 0.6 * nrow(data), replace = TRUE)
training <-data[indices,]
testing  <-data[-indices,]
```

Naively consider all features except the binary at onset for a MLR.
```{r multiModel1, echo=FALSE}
#Naively considering many attributes
multiModel1 <-lm(MEDV ~CRIM+ZN+INDUS+NOX+RM+AGE+DIS+RAD+TAX+PTRATIO+B+LSTAT, data=training)
summary(multiModel1)
```

Some initial insight is that LSTAT and RM indeed were strong predictors. Removing INDUS, AGE, and B, every attribute becomes a significant predictor, depending on alpha. Let's consider what happens if we remove RAD, which varies strongly with TAX.

```{r multiModel2, echo=FALSE}
multiModel2 <-lm(MEDV ~CRIM+ZN+NOX+RM+DIS+TAX+PTRATIO+LSTAT, data=training)
summary(multiModel2)
```

We can see that without RAD, TAX is no longer a strong predictor. As such, TAX adds predictive value in relation to RAD. The next model removes TAX and adds RAD back in.

```{r multiModel3, echo=FALSE}
#Considering all values that are considered 'significant' with RAD included
multiModel3 <-lm(MEDV ~CRIM+ZN+NOX+RM+DIS+RAD+PTRATIO+LSTAT, data=training)
summary(multiModel3)
```

MSE and RMSE for model 1 and 3.
```{r multiModelTesting, echo=TRUE}
multiPredictions <-predict(multiModel1, testing)
RSS <- sum((testing$MEDV - multiPredictions)^2)
MSE1 <- mean((testing$MEDV - multiPredictions)^2)
RMSE1 <- sqrt(MSE1)

MSE1
RMSE1

multiPredictions <-predict(multiModel3, testing)
RSS3 <- sum((testing$MEDV - multiPredictions)^2)
MSE3 <- mean((testing$MEDV - multiPredictions)^2)
RMSE3 <- sqrt(MSE3)

MSE3
RMSE3
```

It seems we are left with 8 features. Lasso and Ridge will give us further insight into parameter selection.

## Ridge Regression

```{r ridgeDataFormatting, echo=FALSE}
#Setting seed
set.seed(1)

#Formatting into matrix
data.mat <- model.matrix(MEDV ~ .-1, data=data)

#delineating columns/features
x <- data.mat[, 1:13]
y <- data[, 'MEDV']

```

Constructing a Ridge model.
```{r ridgeModelStart, echo=TRUE}
#lambda grid
grid <- 10^seq(6, -3, length=10)
#ridgeModel
ridge.mod <- glmnet(scale(x), y, alpha = 0, lambda = grid, thresh = 1e-2, standardize = TRUE)
#Plotting the ridge.mod
plot_glmnet(ridge.mod, xvar = "lambda", label = 13)
```

Considering a best lambda for the model and hyperparameter tuning.
```{r ridgeLambdaTuning, echo=TRUE}
cv.out <- cv.glmnet(x, y, alpha=0, nfolds = 10)
cv.out

plot(cv.out)

best.lambda <- cv.out$lambda.min
best.lambda
```

Considering coefficients for Ridge.
```{r ridgeCoeff, echo=TRUE}

#Viewing coefficients of scaled full ridge model
predict(ridge.mod, type="coefficients", s=best.lambda)

#Selecting above abs(1): NOX, DIS, PTRATIO, LSTAT, RM
newX = data.mat[, c(5,6,8,11,13)]
head(newX)

#Final ridge with all coefficients
ridge.final1 <- glmnet(x, y, alpha = 0, lambda = best.lambda, thresh=1e-2, standardsize = TRUE)
predict(ridge.final1, type="coefficients", s=best.lambda)

#Final ridge with only 5 coefficients
ridge.final2 <- glmnet(newX, y, alpha = 0, lambda = best.lambda, thresh=1e-2, standardsize = TRUE)
predict(ridge.final2, type="coefficients", s=best.lambda)
```

Considering MSE and RMSE for Ridge using all coefficients.
```{r ridgeError, echo=TRUE}

ridge.pred1 <- predict(ridge.final1, s=best.lambda, newx=x)
ridge.MSE = mean((ridge.pred1 -y)^2)
ridge.RMSE = sqrt(mean((ridge.pred1 - y)^2))

ridge.MSE
ridge.RMSE

```

Considering MSE and RMSE for Ridge using only 5 coefficients.
```{r ridgeError2, echo=TRUE}

ridge.pred2 <- predict(ridge.final2, s=best.lambda, newx=newX)
ridge.MSE = mean((ridge.pred2 -y)^2)
ridge.RMSE = sqrt(mean((ridge.pred2 - y)^2))

ridge.MSE
ridge.RMSE

```

Considering R squared for each model manually
```{r ridgeError3, echo=TRUE}

yBar = mean(y)
RSS1 = sum((ridge.pred1 - y)^2)
TSS1 = sum((y - yBar)^2)
rsq1 = 1 - (RSS1/TSS1) 
rsq1

RSS2 = sum((ridge.pred2 - y)^2)
TSS2 = sum((y - yBar)^2)
rsq2 = 1 - (RSS2/TSS2) 
rsq2
```

## Lasso Regression

The Lasso regression will allow us to conduct selection of the most influential independent variables to predict our dependent variable, which is the house price.  
Again, we will use the same range of the regularization parameter, $\lambda$, values, but will sample this range more densely as it proved to provide better results for the parameter selection.
The output of the Lasso regression for various values of $\lambda$ are given below. It is evident that with increasing regularization parameter the number of non-zero regressors is decreasing.    
 
```{r lasso1}
grid <- 10^seq(6, -3, length=100)
lasso.mod <- glmnet(scale(x), y) #default alpha=1
plot_glmnet(lasso.mod, xvar="lambda", label = 12)

```

Now we will conduct 10-fold cross-validation to select optimum regularization parameter lambda. The minimum MSE is produced for $\lambda$ = 0.0307, which keeps 11 non-zero regressors. Increasing $\lambda$ to 0.3789 reduces the number fo regressors to 8.

```{r lasso2}
##cross-validation
lasso.cv.out <- cv.glmnet(scale(x), y, alpha=1, nfolds = 10)
lasso.cv.out
plot(lasso.cv.out)
```
Now we will predict the coefficients of the Lasso regression for 11 non-zero predictors. Also, we will plot the predicted home price versus the true home prices and calculate the MSE and RMSE errors to check how well our mode performs. 

```{r lasso3}
#keep few predictors in lasso
lasso.best.lambda <- lasso.cv.out$lambda[which.max(lasso.cv.out$nzero == 11)]
#lasso.final <- glmnet(scale(x), y, alpha=1, lambda=grid)
lasso.final <- glmnet(x, y, alpha=1, lambda=grid)
predict(lasso.final, type="coefficients", s=lasso.best.lambda )

lasso.pred <- predict(lasso.final, s=lasso.best.lambda, newx=x)
print(paste('MSE:', mean((lasso.pred - y)^2)))
print(paste('RMSE:', sqrt(mean((lasso.pred - y)^2))))
plot(y, lasso.pred,  xlab="True home value, $K", ylab="Lasso prediction, $K",)


```

Considering the R squared for the 11 selector Lasso model.
```{r rSquaredManualLasso1}
yBar = mean(y)
lassoRSS = sum((lasso.pred - y)^2)
lassoTSS = sum((y - yBar)^2)
rsq = 1 - (lassoRSS/lassoTSS) 
rsq
```


Now we will further reduced the number of regressors to keep only 5 most important ones. Below are the coefficients of the Lasso regression and plot of the predicted home price versus the true home prices. We also provide the MSE and RMSE errors. As expected, the scatter of the plot and the MSE/RMSE errors are slightly higher then for the case of 11 regressors, but our model is still doing a decent job at predicting the home values based on half the number of regressors.    

```{r lasso4}
# check for fewer predictors
lasso.best.lambda <- lasso.cv.out$lambda[which.max(lasso.cv.out$nzero == 5)]
lasso.final <- glmnet(x, y, alpha=1, lambda=grid)
predict(lasso.final, type="coefficients", s=lasso.best.lambda )
lasso.pred <- predict(lasso.final, s=lasso.best.lambda, newx=x)
print(paste('MSE:', mean((lasso.pred - y)^2)))
print(paste('RMSE:', sqrt(mean((lasso.pred - y)^2))))
plot(y, lasso.pred, xlab="True home value, $K", ylab="Lasso prediction, $K",)
```

Considering the R squared for the 5 selector Lasso model.
```{r rSquaredManualLasso2}
yBar = mean(y)
lassoRSS = sum((lasso.pred - y)^2)
lassoTSS = sum((y - yBar)^2)
rsq = 1 - (lassoRSS/lassoTSS) 
rsq

```
## K-Fold Cross Validation 

In a standard approach a part fo the data is reserved as a test set and the rest of it is used as a training set. This can lead to a highly variable test error depending on how the dataset was split. Moreover, the machine learning approaches tend to perform  worse when less data are used for training, which happens when the training set is only a part of the entire dataset.    

We are employing a cross-validation (CV) technique as it provides helps to alleviate the above problems. Specifically, we use a K-fold cross validation. In this approach one fold is used as a test set while the remaining K-1 folds are used as the training set. After one of the folds is tested the algorithm moves to the other one. Thus, every data point in the set is a part of the training and the test set for some folds. The final K-fold CV mean squared error (MSE) error is computed as the mean of the MSE of individual folds. 

In practice K = 5 or 10 tend to provide the best results as they yield a test error that does not suffer from excessively high bias, nor from very high variance. We chose to use 10-fold cross-validation approach (K = 10). 

The 10-fold CV MSE is computed for each value of $\lambda$ in the predefined grid. Finally, we chose the $\lambda$ value that minimizes 10-fold CV MSE. For Lasso regression we also test other values of $\lambda$ to select the most important regressors. 


The final MSE's, RMSE's, and $R^2$, achieved by different regression approaches are given in the table below. The number following the method indicates the number of included features.

|            | MSE    | RMSE  |$R^2$|
|------------|--------|-------|-----|
| Multiple 12| 22.554 | 4.749 |0.720|
| Multiple  8| 23.025 | 4.798 |0.695|
| Ridge    13| 22.894 | 4.784 |0.728|
| Ridge     5| 25.096 | 5.009 |0.702|
| Lasso    11| 23.445 | 4.842 |0.722|
| Lasso     5| 28.250 | 5.315 |0.665|


Having more independent variables as an input to regression provides overall smaller MSE and RMSE errors. However, the difference in errors between Ridge regression and Lasso regression with 11 input variables is small. Furthermore, Lasso regression with only 5 most significant regressors still yields  RMSE error, which is only 9.8 % higher compared to the optimum result. Thus, Lasso enables significant reduction in the dataset requirements without loosing much of the prediction accuracy. 

# Summary

The goal was to modell MEDV, the median value of owner occupied homes (in $1,000s), based on given features. MEDV follows a skewed normal distribution with some outliers at its tail. Three regression techniques were performed to model the data: multiple linear regression, Ridge, and Lasso. It was observed that TAX only became a predictor in MLR when RAD was included in, so TAX was removed. Removing insignificant predictors (to alpha) left 8 features. Ridge was performed using all features. Then Ridge was modified to include the 5 greatest predictors. This was determine by taking the greatest absolute value of the coefficients when input was scaled. Lasso was performed with 11 then 5 predictors. Among all methods, LSTAT, RM, and PTRatio remained as significant predictors. Having more independent variables as an input to regression provides overall smaller MSE and RMSE errors. However, the difference in errors between Multiple Ridge and Lasso, each with mostly all input variables (11, 12, or 13), was small. This changed by different amounts when hyperparamater tuning took place.

# Citations

[1] Fang, Julia. “CIS490_LS8_21S_LassoReg&amp;CrossValidation.” MyCourses, 2021,
[linked umassd pdf](https://umassd.umassonline.net/bbcswebdav/pid-700293-dt-content-rid-4661573_1/xid-4661573_1)

[2] Fang, Julia. “R_RidgeLassoCV_Final.pdf” MyCourses, 2021, [linked umassd pdf](https://umassd.umassonline.net/bbcswebdav/pid-700293-dt-content-rid-4655072_1/xid-4655072_1).

[3]Index of /Ml/Machine-Learning-Databases/Housing, [archive.ics.uci.edu/ml/machine-learning-databases/housing/](https://archive.ics.uci.edu/ml/machine-learning-databases/housing/).
